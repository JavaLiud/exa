# 项目功能介绍

本项目是一个基于**多智能体深度强化学习（MADDPG）** 的SAGIN（空间-空中-地面一体化网络）资源分配系统，核心目标是通过分布式智能体协作优化无线网络资源分配（如功率、子载波、无人机位置），满足不同网络切片（高吞吐/H、低延迟/L、广覆盖/C）的差异化需求。主要功能如下：

1. **多智能体协同决策**
   - 6个智能体分别负责TL/NTL层的H/L/C切片，通过局部观测（信道状态、用户位置、数据队列）生成动作（关联决策、功率分配、子载波选择）。
   - 采用集中式Critic训练+分布式Actor执行框架，实现智能体间协同。
2. **分层资源分配机制**
   - **TL层（地面基站）**：优先服务高优先级用户，超载时触发卸载策略。
   - **NTL层（无人机/卫星）**：接收TL层卸载用户，动态调整无人机位置。
3. **真实网络环境建模**
   - 信道模型：莱斯衰落（UAV）、自由空间损耗（LEO）、瑞利衰落（BS）。
   - 用户移动模型：随机方向+边界反弹。
   - 业务模型：L切片数据到达服从泊松分布。
4. **强化学习训练框架**
   - Actor-Critic网络结构（全连接神经网络）。
   - 经验回放缓冲区（ReplayBuffer）稳定训练。
   - 周期性保存模型及训练指标（Loss、Reward）。

------

# 论文来源

《Priority-Based Load Balancing With Multiagent  Deep Reinforcement Learning for  Space–Air–Ground Integrated Network Slicing》

IEEE INTERNET OF THINGS JOURNAL, VOL. 11, NO. 19, 1 OCTOBER 2024

# 论文改动

预计将无人机位置维度在状态中去除，通过数学手段（KKT条件）首先确定无人机高度（最大化服务半径），随后确定无人机平面坐标（最小化无人机与用户的距离）

![image](https://github.com/user-attachments/assets/6da18f6c-c44e-405a-850e-fc63c2d7bc49)


# 项目总结

## 1. 收获与关键技术

- **难点攻克**：
  - **多智能体协同**：通过`Critic`网络共享全局信息解决局部观测局限性，实现动作空间协同（如无人机位置避撞）。
  - **奖励函数设计**：为不同切片定制奖励（H切片奖励吞吐量，L切片惩罚延迟），解决多目标优化冲突。
  - **状态空间对齐**：通过补零统一TL/NTL层观测维度（230维），支持异构智能体训练。
- **软件工程实践**：
  - **模块化设计**：环境（`sagin_env.py`）、智能体（`agent.py`）、网络（`networks.py`）、缓冲区（`buffer.py`）解耦。
  - **可视化监控**：Matplotlib实时绘制Loss/Reward曲线（`main.py`）。
  - **模型持久化**：Checkpoint机制保存/加载模型及经验回放数据。
- **AI与通信结合**：
  - 将信道增益、干扰计算等通信模型（`sagin_env.py`）嵌入RL环境。
  - 使用DRL解决NP-Hard资源分配问题，避免传统优化方法的高计算开销。

## 2. 未来工作展望

- **算法优化**：
  - 引入**注意力机制**改进Critic网络，提升多智能体协作效率。
  - 尝试**分层强化学习**（HRL）解耦长期规划（无人机轨迹）与短期决策（资源分配）。
- **环境增强**：
  - 增加动态障碍物、天气干扰等随机因素，提升仿真真实性。
  - 支持用户数量动态变化，验证算法鲁棒性。
- **部署扩展**：
  - 集成NS-3网络模拟器，验证算法在真实协议栈下的性能。
  - 探索联邦学习框架，实现分布式网络节点的协同训练。
- **性能提升**：
  - 采用**图神经网络（GNN）** 建模网络拓扑关系，替代全连接网络。
  - 设计课程学习（Curriculum Learning）策略，逐步提升环境复杂度。
